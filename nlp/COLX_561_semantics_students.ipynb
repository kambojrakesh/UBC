{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c2a3d7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['n', 'v', 'a', 'r']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "pos_list = [wn.NOUN, wn.VERB, wn.ADJ, wn.ADV]\n",
    "\n",
    "pos_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a10ad4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82115\n",
      "[Synset('entity.n.01'), Synset('physical_entity.n.01'), Synset('abstraction.n.06'), Synset('thing.n.12'), Synset('object.n.01'), Synset('whole.n.02'), Synset('congener.n.03'), Synset('living_thing.n.01'), Synset('organism.n.01'), Synset('benthos.n.02')]\n",
      "13767\n",
      "[Synset('breathe.v.01'), Synset('respire.v.02'), Synset('respire.v.01'), Synset('choke.v.01'), Synset('hyperventilate.v.02'), Synset('hyperventilate.v.01'), Synset('aspirate.v.03'), Synset('burp.v.01'), Synset('force_out.v.08'), Synset('hiccup.v.01')]\n",
      "18156\n",
      "[Synset('able.a.01'), Synset('unable.a.01'), Synset('abaxial.a.01'), Synset('adaxial.a.01'), Synset('acroscopic.a.01'), Synset('basiscopic.a.01'), Synset('abducent.a.01'), Synset('adducent.a.01'), Synset('nascent.a.01'), Synset('emergent.s.02')]\n",
      "3621\n",
      "[Synset('a_cappella.r.01'), Synset('ad.r.01'), Synset('ce.r.01'), Synset('bc.r.01'), Synset('bce.r.01'), Synset('horseback.r.01'), Synset('barely.r.01'), Synset('just.r.06'), Synset('hardly.r.02'), Synset('anisotropically.r.01')]\n"
     ]
    }
   ],
   "source": [
    "for pos in pos_list:\n",
    "    synsets = list(wn.all_synsets(pos))\n",
    "    print(len(synsets))\n",
    "    print(synsets[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "444be583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('goal.n.01'),\n",
       " Synset('finish.n.04'),\n",
       " Synset('goal.n.03'),\n",
       " Synset('goal.n.04')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets(\"goal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da4d8a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_synonyms(word):\n",
    "    synsets = wn.synsets(word)\n",
    "    #synsets = [1,2,3,4,5]\n",
    "    for i in range(0, len(synsets)):\n",
    "            for j in range(i+1, len(synsets)):\n",
    "                print(synsets[i], synsets[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c22802ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('goal.n.01'),\n",
       " Synset('finish.n.04'),\n",
       " Synset('goal.n.03'),\n",
       " Synset('goal.n.04')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goal_synsets = wn.synsets(\"goal\")\n",
    "goal_synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06b1a2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- goal.n.01\n",
      "============= the state of affairs that a plan is intended to achieve and that (when achieved) terminates behavior intended to achieve it\n",
      "+++++++++++++ ['the ends justify the means']\n",
      "===========================================================================================================================\n",
      "------- finish.n.04\n",
      "============= the place designated as the end (as of a race or journey)\n",
      "+++++++++++++ ['a crowd assembled at the finish', 'he was nearly exhausted as their destination came into view']\n",
      "===========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for synset in goal_synsets[:2]:\n",
    "    print(\"-------\",synset.name())\n",
    "    print(\"=============\",synset.definition())\n",
    "    print(\"+++++++++++++\",synset.examples())\n",
    "    print('===========================================================================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4e39e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('goal.n.01.goal'), Lemma('goal.n.01.end')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goal_synsets[0].lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e748fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2e5935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5b24573",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dcd0e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package semcor to\n",
      "[nltk_data]     C:\\Users\\Vikki\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package semcor is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tree('DT', ['The']), Tree(Lemma('group.n.01.group'), [Tree('NE', [Tree('NNP', ['Fulton', 'County', 'Grand', 'Jury'])])]), Tree(Lemma('state.v.01.say'), [Tree('VB', ['said'])]), Tree(Lemma('friday.n.01.Friday'), [Tree('NN', ['Friday'])]), Tree('DT', ['an']), Tree(Lemma('probe.n.01.investigation'), [Tree('NN', ['investigation'])]), Tree('IN', ['of']), Tree(Lemma('atlanta.n.01.Atlanta'), [Tree('NN', ['Atlanta'])]), Tree('POS', [\"'s\"]), Tree(Lemma('late.s.03.recent'), [Tree('JJ', ['recent'])]), Tree(Lemma('primary.n.01.primary_election'), [Tree('NN', ['primary', 'election'])]), Tree(Lemma('produce.v.04.produce'), [Tree('VB', ['produced'])]), Tree(None, ['``']), Tree('DT', ['no']), Tree(Lemma('evidence.n.01.evidence'), [Tree('NN', ['evidence'])]), Tree(None, [\"''\"]), Tree('IN', ['that']), Tree('DT', ['any']), Tree(Lemma('abnormality.n.04.irregularity'), [Tree('NN', ['irregularities'])]), Tree(Lemma('happen.v.01.take_place'), [Tree('VB', ['took', 'place'])]), Tree(None, ['.'])]\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "defaultdict(<class 'int'>, {'interest.n.01': 57, 'sake.n.01': 32, 'interest.n.03': 20, 'matter_to.v.01': 1, 'pastime.n.01': 3, 'interest.n.04': 14, 'interest.n.06': 5, 'interest.v.01': 5, 'concern.v.02': 2, 'interest.n.05': 7})\n"
     ]
    }
   ],
   "source": [
    "#provided code\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "nltk.download('semcor')\n",
    "from nltk.corpus import semcor\n",
    "wanted_word = \"interest\"\n",
    "\n",
    "\n",
    "sense_counts = defaultdict(int)\n",
    "\n",
    "print(semcor.tagged_sents(tag='sense')[0])\n",
    "\n",
    "for sent in semcor.tagged_sents(tag='sense'):\n",
    "    for chunk in sent:\n",
    "        lemma = chunk.label()\n",
    "        try:\n",
    "            lemma_name = lemma.name()\n",
    "        except:\n",
    "            lemma_name = None\n",
    "        if lemma_name == wanted_word:\n",
    "            sense_counts[lemma.synset().name()] += 1\n",
    "            if sum(sense_counts.values()) % 10 == 0:\n",
    "                print(sum(sense_counts.values()))\n",
    "                \n",
    "print(sense_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96044924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #provided code\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.feature_extraction import DictVectorizer\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# import numpy as np\n",
    "\n",
    "# correct_senses = []\n",
    "# feature_dicts = []\n",
    "\n",
    "# for sent in semcor.tagged_sents(tag='sense'):\n",
    "#     for i in range(len(sent)):\n",
    "#         chunk = sent[i]\n",
    "#         lemma = chunk.label()\n",
    "#         try: \n",
    "#             lemma_name = lemma.name()\n",
    "#         except:\n",
    "#             lemma_name = None\n",
    "#         if lemma_name == wanted_word:\n",
    "#             correct_senses.append(lemma.synset().name())\n",
    "#             feature_dict = {}\n",
    "#             if i > 0:\n",
    "#                 feature_dict[\"BEFORE_\" + sent[i-1].leaves()[-1].lower()] = 1\n",
    "#             if i < len(sent):\n",
    "#                 feature_dict[\"AFTER_\" + sent[i+1].leaves()[-1].lower()] = 1\n",
    "#             feature_dicts.append(feature_dict)\n",
    "\n",
    "\n",
    "# vectorizer = DictVectorizer()\n",
    "# X = vectorizer.fit_transform(feature_dicts)\n",
    "# clf = DecisionTreeClassifier(max_depth=3)\n",
    "# print(np.mean(cross_val_score(clf,X,correct_senses,cv=5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91feee8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3289a520",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d43f127",
   "metadata": {},
   "outputs": [],
   "source": [
    "#correct_senses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19ea487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c10883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5433ee35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9563d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Calculate the {Wu-Palmer/Path} similarity for the following nodes (tree given)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01cc8cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wu_palmer_similarity :  0.6\n",
      "path_similarity :  0.1111111111111111\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "synset1 = wordnet.synset('dog.n.01')\n",
    "synset2 = wordnet.synset('dog.n.02')\n",
    "\n",
    "similarity = synset1.wup_similarity(synset2)\n",
    "print(\"wu_palmer_similarity : \" , similarity)\n",
    "\n",
    "similarity = synset1.path_similarity(synset2)\n",
    "print(\"path_similarity : \" , similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac847c6",
   "metadata": {},
   "source": [
    "#Write a function that sorts synonyms based on their Wu-Palmer similarity.  ie, it takes a word as input, finds its synsets, and then sorts them by their WP similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5323923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frump.n.01: 1.00\n",
      "dog.n.03: 1.00\n",
      "cad.n.01: 1.00\n",
      "frank.n.02: 1.00\n",
      "pawl.n.01: 1.00\n",
      "andiron.n.01: 1.00\n",
      "chase.v.01: 1.00\n",
      "dog.n.01: 0.93\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "def sort_synonyms_by_wp_similarity(word):\n",
    "    synsets = wordnet.synsets(word)\n",
    "\n",
    "    similarities = [(synset, synset.wup_similarity(synset)) for synset in synsets]\n",
    "\n",
    "    similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "    return similarities\n",
    "\n",
    "similarities = sort_synonyms_by_wp_similarity(\"dog\")\n",
    "\n",
    "\n",
    "for synset, similarity in similarities:\n",
    "    print(f'{synset.name()}: {similarity:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d8cfc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "18d27131",
   "metadata": {},
   "source": [
    "# Write a function that transforms a sentence into a feature vector containing the following features: the first word before and after the word of interest that isn't a stopword, and any verbs in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4c32a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'beautiful', 'is']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def sentence_to_feature_vector(sentence, target_word):\n",
    "    # Tokenize the sentence\n",
    "    tokens = word_tokenize(sentence)\n",
    "    # Get the index of the target word\n",
    "    target_index = tokens.index(target_word)\n",
    "    # Get the first non-stopword before and after the target word\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    before_word = None\n",
    "    after_word = None\n",
    "    for i in range(target_index-1, -1, -1):\n",
    "        if tokens[i] not in stop_words:\n",
    "            before_word = tokens[i]\n",
    "            break\n",
    "    for i in range(target_index+1, len(tokens)):\n",
    "        if tokens[i] not in stop_words:\n",
    "            after_word = tokens[i]\n",
    "            break\n",
    "    # Get all verbs in the sentence\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    verbs = [word for word, pos in pos_tags if pos.startswith('VB')]\n",
    "    # Return the feature vector\n",
    "    return [before_word, after_word] + verbs\n",
    "\n",
    "sentence = \"The ocean is so beautiful and peaceful.\"\n",
    "target_word = \"ocean\"\n",
    "feature_vector = sentence_to_feature_vector(sentence, target_word)\n",
    "print(feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a0670a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f84585b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8c536e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accf5b70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3bc895e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goal\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Get the synset for the word 'goal'\n",
    "goal_synsets = wordnet.synsets('goal')\n",
    "\n",
    "# Get the lemmas for the first synset in the list\n",
    "lemmas = goal_synsets[0].lemmas()\n",
    "\n",
    "# Print the name of each lemma\n",
    "for lemma in lemmas:\n",
    "    print(lemma.name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9a2a657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('finish.n.04')\n"
     ]
    }
   ],
   "source": [
    "print(goal_synsets[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "373bdc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word.n.01: 1.00\n",
      "word.n.02: 1.00\n",
      "news.n.01: 1.00\n",
      "word.n.04: 1.00\n",
      "discussion.n.02: 1.00\n",
      "parole.n.01: 1.00\n",
      "word.n.07: 1.00\n",
      "son.n.02: 1.00\n",
      "password.n.01: 1.00\n",
      "bible.n.01: 1.00\n",
      "give_voice.v.01: 1.00\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "def sort_synonyms_by_wp_similarity(word):\n",
    "    synsets = wordnet.synsets(word)\n",
    "\n",
    "    similarities = [(synset, synset.wup_similarity(synset)) for synset in synsets]\n",
    "\n",
    "    similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "    return similarities\n",
    "\n",
    "similarities = sort_synonyms_by_wp_similarity(\"word\")\n",
    "\n",
    "\n",
    "for synset, similarity in similarities:\n",
    "    print(f'{synset.name()}: {similarity:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf954d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word.n.01: 1.00\n",
      "word.n.02: 1.00\n",
      "news.n.01: 1.00\n",
      "word.n.04: 1.00\n",
      "discussion.n.02: 1.00\n",
      "parole.n.01: 1.00\n",
      "word.n.07: 1.00\n",
      "son.n.02: 1.00\n",
      "password.n.01: 1.00\n",
      "bible.n.01: 1.00\n",
      "give_voice.v.01: 1.00\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def sort_synonyms_by_wp_similarity(word):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    synsets = wordnet.synsets(lemmatizer.lemmatize(word))\n",
    "\n",
    "    similarities = [(synset, synset.wup_similarity(synset)) for synset in synsets]\n",
    "\n",
    "    similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "    return similarities\n",
    "\n",
    "similarities = sort_synonyms_by_wp_similarity(\"word\")\n",
    "\n",
    "for synset, similarity in similarities:\n",
    "    print(f'{synset.name()}: {similarity:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4393256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9af715b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b23be06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['swim', '.', 'love', 'swim', 'is']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def sentence_to_feature_vector(sentence, target_word):\n",
    "\n",
    "    tokens = word_tokenize(sentence)\n",
    " \n",
    "    target_index = tokens.index(target_word)\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    before_word = None\n",
    "    after_word = None\n",
    "    for i in range(target_index-1, -1, -1):\n",
    "        if tokens[i] not in stop_words:\n",
    "            before_word = tokens[i]\n",
    "            break\n",
    "    for i in range(target_index+1, len(tokens)):\n",
    "        if tokens[i] not in stop_words:\n",
    "            after_word = tokens[i]\n",
    "            break\n",
    "    # Get all verbs in the sentence\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    verbs = [word for word, pos in pos_tags if pos.startswith('VB')]\n",
    "\n",
    "    return [before_word, after_word] + verbs\n",
    "\n",
    "sentence = \"I love to swim in the ocean. The ocean is so beautiful and peaceful.\"\n",
    "target_word = \"ocean\"\n",
    "feature_vector = sentence_to_feature_vector(sentence, target_word)\n",
    "print(feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e198dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
