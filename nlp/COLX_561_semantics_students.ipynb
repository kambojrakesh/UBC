{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c2a3d7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['n', 'v', 'a', 'r']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "pos_list = [wn.NOUN, wn.VERB, wn.ADJ, wn.ADV]\n",
    "\n",
    "pos_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a10ad4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82115\n",
      "[Synset('entity.n.01'), Synset('physical_entity.n.01'), Synset('abstraction.n.06'), Synset('thing.n.12'), Synset('object.n.01'), Synset('whole.n.02'), Synset('congener.n.03'), Synset('living_thing.n.01'), Synset('organism.n.01'), Synset('benthos.n.02')]\n",
      "13767\n",
      "[Synset('breathe.v.01'), Synset('respire.v.02'), Synset('respire.v.01'), Synset('choke.v.01'), Synset('hyperventilate.v.02'), Synset('hyperventilate.v.01'), Synset('aspirate.v.03'), Synset('burp.v.01'), Synset('force_out.v.08'), Synset('hiccup.v.01')]\n",
      "18156\n",
      "[Synset('able.a.01'), Synset('unable.a.01'), Synset('abaxial.a.01'), Synset('adaxial.a.01'), Synset('acroscopic.a.01'), Synset('basiscopic.a.01'), Synset('abducent.a.01'), Synset('adducent.a.01'), Synset('nascent.a.01'), Synset('emergent.s.02')]\n",
      "3621\n",
      "[Synset('a_cappella.r.01'), Synset('ad.r.01'), Synset('ce.r.01'), Synset('bc.r.01'), Synset('bce.r.01'), Synset('horseback.r.01'), Synset('barely.r.01'), Synset('just.r.06'), Synset('hardly.r.02'), Synset('anisotropically.r.01')]\n"
     ]
    }
   ],
   "source": [
    "for pos in pos_list:\n",
    "    synsets = list(wn.all_synsets(pos))\n",
    "    print(len(synsets))\n",
    "    print(synsets[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "444be583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('goal.n.01'),\n",
       " Synset('finish.n.04'),\n",
       " Synset('goal.n.03'),\n",
       " Synset('goal.n.04')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets(\"goal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da4d8a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_synonyms(word):\n",
    "    synsets = wn.synsets(word)\n",
    "    #synsets = [1,2,3,4,5]\n",
    "    for i in range(0, len(synsets)):\n",
    "            for j in range(i+1, len(synsets)):\n",
    "                print(synsets[i], synsets[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c22802ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('goal.n.01'),\n",
       " Synset('finish.n.04'),\n",
       " Synset('goal.n.03'),\n",
       " Synset('goal.n.04')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goal_synsets = wn.synsets(\"goal\")\n",
    "goal_synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "06b1a2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- goal.n.01\n",
      "============= the state of affairs that a plan is intended to achieve and that (when achieved) terminates behavior intended to achieve it\n",
      "+++++++++++++ ['the ends justify the means']\n",
      "===========================================================================================================================\n",
      "------- finish.n.04\n",
      "============= the place designated as the end (as of a race or journey)\n",
      "+++++++++++++ ['a crowd assembled at the finish', 'he was nearly exhausted as their destination came into view']\n",
      "===========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for synset in goal_synsets[:2]:\n",
    "    print(\"-------\",synset.name())\n",
    "    print(\"=============\",synset.definition())\n",
    "    print(\"+++++++++++++\",synset.examples())\n",
    "    print('===========================================================================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a4e39e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('goal.n.01.goal'), Lemma('goal.n.01.end')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goal_synsets[0].lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e748fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2e5935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5b24573",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dcd0e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package semcor to\n",
      "[nltk_data]     C:\\Users\\Vikki\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package semcor is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tree('DT', ['The']), Tree(Lemma('group.n.01.group'), [Tree('NE', [Tree('NNP', ['Fulton', 'County', 'Grand', 'Jury'])])]), Tree(Lemma('state.v.01.say'), [Tree('VB', ['said'])]), Tree(Lemma('friday.n.01.Friday'), [Tree('NN', ['Friday'])]), Tree('DT', ['an']), Tree(Lemma('probe.n.01.investigation'), [Tree('NN', ['investigation'])]), Tree('IN', ['of']), Tree(Lemma('atlanta.n.01.Atlanta'), [Tree('NN', ['Atlanta'])]), Tree('POS', [\"'s\"]), Tree(Lemma('late.s.03.recent'), [Tree('JJ', ['recent'])]), Tree(Lemma('primary.n.01.primary_election'), [Tree('NN', ['primary', 'election'])]), Tree(Lemma('produce.v.04.produce'), [Tree('VB', ['produced'])]), Tree(None, ['``']), Tree('DT', ['no']), Tree(Lemma('evidence.n.01.evidence'), [Tree('NN', ['evidence'])]), Tree(None, [\"''\"]), Tree('IN', ['that']), Tree('DT', ['any']), Tree(Lemma('abnormality.n.04.irregularity'), [Tree('NN', ['irregularities'])]), Tree(Lemma('happen.v.01.take_place'), [Tree('VB', ['took', 'place'])]), Tree(None, ['.'])]\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "defaultdict(<class 'int'>, {'interest.n.01': 57, 'sake.n.01': 32, 'interest.n.03': 20, 'matter_to.v.01': 1, 'pastime.n.01': 3, 'interest.n.04': 14, 'interest.n.06': 5, 'interest.v.01': 5, 'concern.v.02': 2, 'interest.n.05': 7})\n"
     ]
    }
   ],
   "source": [
    "#provided code\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "nltk.download('semcor')\n",
    "from nltk.corpus import semcor\n",
    "wanted_word = \"interest\"\n",
    "\n",
    "\n",
    "sense_counts = defaultdict(int)\n",
    "\n",
    "print(semcor.tagged_sents(tag='sense')[0])\n",
    "\n",
    "for sent in semcor.tagged_sents(tag='sense'):\n",
    "    for chunk in sent:\n",
    "        lemma = chunk.label()\n",
    "        try:\n",
    "            lemma_name = lemma.name()\n",
    "        except:\n",
    "            lemma_name = None\n",
    "        if lemma_name == wanted_word:\n",
    "            sense_counts[lemma.synset().name()] += 1\n",
    "            if sum(sense_counts.values()) % 10 == 0:\n",
    "                print(sum(sense_counts.values()))\n",
    "                \n",
    "print(sense_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96044924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5549425287356322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vikki\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#provided code\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "correct_senses = []\n",
    "feature_dicts = []\n",
    "\n",
    "for sent in semcor.tagged_sents(tag='sense'):\n",
    "    for i in range(len(sent)):\n",
    "        chunk = sent[i]\n",
    "        lemma = chunk.label()\n",
    "        try: \n",
    "            lemma_name = lemma.name()\n",
    "        except:\n",
    "            lemma_name = None\n",
    "        if lemma_name == wanted_word:\n",
    "            correct_senses.append(lemma.synset().name())\n",
    "            feature_dict = {}\n",
    "            if i > 0:\n",
    "                feature_dict[\"BEFORE_\" + sent[i-1].leaves()[-1].lower()] = 1\n",
    "            if i < len(sent):\n",
    "                feature_dict[\"AFTER_\" + sent[i+1].leaves()[-1].lower()] = 1\n",
    "            feature_dicts.append(feature_dict)\n",
    "\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "X = vectorizer.fit_transform(feature_dicts)\n",
    "clf = DecisionTreeClassifier(max_depth=3)\n",
    "print(np.mean(cross_val_score(clf,X,correct_senses,cv=5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91feee8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3289a520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'BEFORE_widespread': 1, 'AFTER_in': 1},\n",
       " {'BEFORE_best': 1, 'AFTER_of': 1},\n",
       " {'BEFORE_much': 1, 'AFTER_shown': 1},\n",
       " {'BEFORE_musical': 1, 'AFTER_survived': 1},\n",
       " {'BEFORE_water': 1, 'AFTER_,': 1},\n",
       " {'BEFORE_of': 1, 'AFTER_and': 1},\n",
       " {'BEFORE_and': 1, 'AFTER_in': 1},\n",
       " {'BEFORE_recreation': 1, 'AFTER_already': 1},\n",
       " {'BEFORE_water': 1, 'AFTER_,': 1},\n",
       " {'BEFORE_other': 1, 'AFTER_.': 1},\n",
       " {'BEFORE_offer': 1, 'AFTER_in': 1},\n",
       " {'BEFORE_these': 1, 'AFTER_and': 1},\n",
       " {'BEFORE_water': 1, 'AFTER_is': 1},\n",
       " {'BEFORE_of': 1, 'AFTER_at': 1},\n",
       " {'BEFORE_high': 1, 'AFTER_are': 1},\n",
       " {'BEFORE_additional': 1, 'AFTER_to': 1},\n",
       " {'BEFORE_another': 1, 'AFTER_.': 1},\n",
       " {'BEFORE_fishing': 1, 'AFTER_for': 1},\n",
       " {'BEFORE_add': 1, 'AFTER_at': 1},\n",
       " {'BEFORE_historical': 1, 'AFTER_with': 1},\n",
       " {'BEFORE_gets': 1, 'AFTER_across': 1},\n",
       " {'BEFORE_geological': 1, 'AFTER_to': 1},\n",
       " {'BEFORE_of': 1, 'AFTER_.': 1},\n",
       " {'BEFORE_recreation': 1, 'AFTER_,': 1},\n",
       " {'BEFORE_and': 1, 'AFTER_,': 1},\n",
       " {'BEFORE_international': 1, 'AFTER_,': 1},\n",
       " {'BEFORE_artistic': 1, 'AFTER_,': 1},\n",
       " {'BEFORE_of': 1, 'AFTER_may': 1},\n",
       " {'BEFORE_irrelevant': 1, 'AFTER_aroused': 1},\n",
       " {'BEFORE_of': 1, \"AFTER_''\": 1},\n",
       " {'BEFORE_and': 1, 'AFTER_of': 1},\n",
       " {'BEFORE_general': 1, 'AFTER_to': 1},\n",
       " {'BEFORE_including': 1, 'AFTER_at': 1},\n",
       " {'BEFORE_sufficient': 1, 'AFTER_in': 1},\n",
       " {'BEFORE_with': 1, 'AFTER_.': 1},\n",
       " {'BEFORE_and': 1, 'AFTER_,': 1},\n",
       " {'BEFORE_structural': 1, 'AFTER_.': 1},\n",
       " {'BEFORE_the': 1, 'AFTER_of': 1},\n",
       " {'BEFORE_of': 1, 'AFTER_that': 1},\n",
       " {'BEFORE_of': 1, 'AFTER_.': 1},\n",
       " {'BEFORE_of': 1, 'AFTER_will': 1},\n",
       " {'BEFORE_national': 1, 'AFTER_.': 1},\n",
       " {'BEFORE_political': 1, 'AFTER_supported': 1},\n",
       " {'BEFORE_national': 1, 'AFTER_had': 1},\n",
       " {'BEFORE_commercial': 1, 'AFTER_,': 1},\n",
       " {'BEFORE_unusual': 1, 'AFTER_in': 1},\n",
       " {'BEFORE_with': 1, 'AFTER_do': 1},\n",
       " {'BEFORE_vigorous': 1, 'AFTER_with': 1},\n",
       " {'BEFORE_extreme': 1, 'AFTER_.': 1},\n",
       " {'BEFORE_high': 1, 'AFTER_mortgages': 1},\n",
       " {'BEFORE_without': 1, 'AFTER_at': 1},\n",
       " {'BEFORE_of': 1, 'AFTER_.': 1},\n",
       " {'BEFORE_,': 1, 'AFTER_him': 1},\n",
       " {'BEFORE_vague': 1, 'AFTER_;': 1},\n",
       " {'BEFORE_the': 1, 'AFTER_was': 1},\n",
       " {'BEFORE_enormous': 1, 'AFTER_,': 1},\n",
       " {'BEFORE_his': 1, 'AFTER_.': 1},\n",
       " {'BEFORE_spectator': 1, 'AFTER_.': 1},\n",
       " {'BEFORE_professional': 1, 'AFTER_have': 1},\n",
       " {'BEFORE_intelligent': 1, 'AFTER_that': 1},\n",
       " {'BEFORE_active': 1, 'AFTER_in': 1},\n",
       " {'BEFORE_lasting': 1, 'AFTER_in': 1},\n",
       " {'BEFORE_the': 1, 'AFTER_of': 1},\n",
       " {'BEFORE_not': 1, 'AFTER_in': 1},\n",
       " {'BEFORE_be': 1, 'AFTER_only': 1},\n",
       " {'BEFORE_pretended': 1, 'AFTER_-': 1},\n",
       " {'BEFORE_his': 1, 'AFTER_in': 1},\n",
       " {'BEFORE_broad': 1, 'AFTER_in': 1},\n",
       " {'BEFORE_his': 1, 'AFTER_in': 1},\n",
       " {'BEFORE_plus': 1, 'AFTER_,': 1},\n",
       " {'BEFORE_and': 1, 'AFTER_of': 1},\n",
       " {'BEFORE_be': 1, 'AFTER_and': 1},\n",
       " {'BEFORE_in': 1, 'AFTER_if': 1},\n",
       " {'BEFORE_english': 1, 'AFTER_in': 1},\n",
       " {'BEFORE_new': 1, 'AFTER_for': 1},\n",
       " {'BEFORE_the': 1, 'AFTER_of': 1},\n",
       " {'BEFORE_soviet': 1, 'AFTER_.': 1},\n",
       " {'BEFORE_national': 1, 'AFTER_,': 1},\n",
       " {'BEFORE_the': 1, 'AFTER_of': 1},\n",
       " {'BEFORE_continuing': 1, 'AFTER_of': 1},\n",
       " {'BEFORE_whose': 1, 'AFTER_in': 1},\n",
       " {'BEFORE_of': 1, 'AFTER_on': 1},\n",
       " {'BEFORE_religious': 1, 'AFTER_seems': 1},\n",
       " {'BEFORE_productive': 1, 'AFTER_.': 1},\n",
       " {'BEFORE_of': 1, 'AFTER_pursued': 1},\n",
       " {'BEFORE_and': 1, 'AFTER_in': 1},\n",
       " {'BEFORE_best': 1, 'AFTER_of': 1},\n",
       " {'BEFORE_best': 1, 'AFTER_really': 1},\n",
       " {'BEFORE_in': 1, 'AFTER_for': 1},\n",
       " {'BEFORE_and': 1, 'AFTER_:': 1},\n",
       " {'BEFORE_extreme': 1, 'AFTER_as': 1},\n",
       " {'BEFORE_public': 1, 'AFTER_is': 1},\n",
       " {'BEFORE_national': 1, 'AFTER_.': 1},\n",
       " {'BEFORE_not': 1, 'AFTER_them': 1},\n",
       " {'BEFORE_her': 1, 'AFTER_with': 1},\n",
       " {'BEFORE_his': 1, 'AFTER_,': 1},\n",
       " {'BEFORE_common': 1, 'AFTER_in': 1},\n",
       " {'BEFORE_intrinsic': 1, 'AFTER_and': 1},\n",
       " {'BEFORE_and': 1, 'AFTER_in': 1},\n",
       " {'BEFORE_the': 1, 'AFTER_of': 1},\n",
       " {'BEFORE_the': 1, 'AFTER_of': 1},\n",
       " {'BEFORE_accrued': 1, 'AFTER_on': 1},\n",
       " {'BEFORE_bear': 1, 'AFTER_,': 1},\n",
       " {'BEFORE_his': 1, 'AFTER_may': 1},\n",
       " {'BEFORE_his': 1, 'AFTER_may': 1},\n",
       " {'BEFORE_accrued': 1, 'AFTER_on': 1},\n",
       " {'BEFORE_bear': 1, 'AFTER_.': 1},\n",
       " {'BEFORE_public': 1, 'AFTER_``': 1},\n",
       " {'BEFORE_public': 1, 'AFTER_?': 1},\n",
       " {'BEFORE_best': 1, 'AFTER_of': 1},\n",
       " {'BEFORE_the': 1, 'AFTER_of': 1},\n",
       " {'BEFORE_stock': 1, 'AFTER_.': 1},\n",
       " {'BEFORE_stock': 1, 'AFTER_and': 1},\n",
       " {'BEFORE_best': 1, 'AFTER_of': 1},\n",
       " {'BEFORE_best': 1, 'AFTER_of': 1},\n",
       " {'BEFORE_public': 1, 'AFTER_to': 1},\n",
       " {'BEFORE_the': 1, 'AFTER_of': 1},\n",
       " {'BEFORE_involved': 1, 'AFTER_of': 1},\n",
       " {'BEFORE_public': 1, \"AFTER_''\": 1},\n",
       " {'BEFORE_passing': 1, 'AFTER_or': 1},\n",
       " {'BEFORE_american': 1, 'AFTER_abroad': 1},\n",
       " {'BEFORE_private': 1, 'AFTER_sufficiently': 1},\n",
       " {'AFTER_at': 1},\n",
       " {'BEFORE_such': 1, 'AFTER_must': 1},\n",
       " {'BEFORE_little': 1, 'AFTER_in': 1},\n",
       " {'BEFORE_of': 1, 'AFTER_are': 1},\n",
       " {'BEFORE_pedantic': 1, 'AFTER_but': 1},\n",
       " {'BEFORE_great': 1, 'AFTER_,': 1},\n",
       " {'BEFORE_national': 1, 'AFTER_in': 1},\n",
       " {'BEFORE_the': 1, 'AFTER_of': 1},\n",
       " {'BEFORE_an': 1, 'AFTER_in': 1},\n",
       " {'BEFORE_public': 1, 'AFTER_would': 1},\n",
       " {'BEFORE_public': 1, 'AFTER_.': 1},\n",
       " {\"BEFORE_'s\": 1, 'AFTER_to': 1},\n",
       " {'BEFORE_the': 1, 'AFTER_of': 1},\n",
       " {'BEFORE_the': 1, 'AFTER_of': 1},\n",
       " {'BEFORE_special': 1, 'AFTER_of': 1},\n",
       " {'BEFORE_various': 1, 'AFTER_groups': 1},\n",
       " {'BEFORE_enough': 1, 'AFTER_in': 1},\n",
       " {'BEFORE_main': 1, 'AFTER_was': 1},\n",
       " {'BEFORE_his': 1, 'AFTER_was': 1},\n",
       " {'BEFORE_no': 1, 'AFTER_all': 1},\n",
       " {'BEFORE_special': 1, 'AFTER_.': 1},\n",
       " {'BEFORE_dancers': 1, 'AFTER_in': 1},\n",
       " {'BEFORE_seriously': 1, 'AFTER_in': 1},\n",
       " {'BEFORE_,': 1, 'AFTER_you': 1}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d43f127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['interest.n.01',\n",
       " 'sake.n.01',\n",
       " 'interest.n.01',\n",
       " 'interest.n.01',\n",
       " 'interest.n.03',\n",
       " 'interest.n.03',\n",
       " 'matter_to.v.01',\n",
       " 'interest.n.03',\n",
       " 'interest.n.03',\n",
       " 'interest.n.03',\n",
       " 'interest.n.03',\n",
       " 'interest.n.03',\n",
       " 'interest.n.03',\n",
       " 'interest.n.01',\n",
       " 'interest.n.01',\n",
       " 'interest.n.01',\n",
       " 'interest.n.01',\n",
       " 'interest.n.01',\n",
       " 'interest.n.03',\n",
       " 'interest.n.03',\n",
       " 'interest.n.01',\n",
       " 'interest.n.03',\n",
       " 'interest.n.01',\n",
       " 'interest.n.03',\n",
       " 'interest.n.03',\n",
       " 'pastime.n.01',\n",
       " 'interest.n.01',\n",
       " 'interest.n.01',\n",
       " 'interest.n.01',\n",
       " 'interest.n.01',\n",
       " 'sake.n.01',\n",
       " 'interest.n.03',\n",
       " 'interest.n.04',\n",
       " 'interest.n.01',\n",
       " 'interest.n.04',\n",
       " 'interest.n.04',\n",
       " 'interest.n.01',\n",
       " 'sake.n.01',\n",
       " 'interest.n.01',\n",
       " 'interest.n.01',\n",
       " 'interest.n.01',\n",
       " 'sake.n.01',\n",
       " 'interest.n.06',\n",
       " 'sake.n.01',\n",
       " 'sake.n.01',\n",
       " 'interest.n.01',\n",
       " 'interest.n.01',\n",
       " 'interest.n.01',\n",
       " 'interest.n.01',\n",
       " 'interest.n.04',\n",
       " 'interest.n.03',\n",
       " 'interest.n.01',\n",
       " 'interest.v.01',\n",
       " 'interest.n.01',\n",
       " 'interest.n.04',\n",
       " 'interest.n.01',\n",
       " 'interest.n.01',\n",
       " 'interest.n.01',\n",
       " 'interest.n.01',\n",
       " 'interest.n.01',\n",
       " 'interest.n.01',\n",
       " 'interest.n.01',\n",
       " 'sake.n.01',\n",
       " 'concern.v.02',\n",
       " 'concern.v.02',\n",
       " 'interest.n.01',\n",
       " 'interest.n.01',\n",
       " 'interest.n.01',\n",
       " 'interest.n.01',\n",
       " 'interest.n.04',\n",
       " 'pastime.n.01',\n",
       " 'interest.n.04',\n",
       " 'interest.n.04',\n",
       " 'interest.n.01',\n",
       " 'interest.n.03',\n",
       " 'interest.n.01',\n",
       " 'sake.n.01',\n",
       " 'sake.n.01',\n",
       " 'sake.n.01',\n",
       " 'interest.n.01',\n",
       " 'interest.n.01',\n",
       " 'interest.n.01',\n",
       " 'interest.n.01',\n",
       " 'interest.n.06',\n",
       " 'interest.n.01',\n",
       " 'interest.n.01',\n",
       " 'sake.n.01',\n",
       " 'sake.n.01',\n",
       " 'interest.n.03',\n",
       " 'interest.n.01',\n",
       " 'interest.n.01',\n",
       " 'sake.n.01',\n",
       " 'sake.n.01',\n",
       " 'interest.v.01',\n",
       " 'interest.n.01',\n",
       " 'interest.n.01',\n",
       " 'interest.n.01',\n",
       " 'interest.n.03',\n",
       " 'interest.n.05',\n",
       " 'sake.n.01',\n",
       " 'sake.n.01',\n",
       " 'interest.n.04',\n",
       " 'interest.n.04',\n",
       " 'interest.n.05',\n",
       " 'interest.n.05',\n",
       " 'interest.n.04',\n",
       " 'interest.n.04',\n",
       " 'sake.n.01',\n",
       " 'sake.n.01',\n",
       " 'sake.n.01',\n",
       " 'sake.n.01',\n",
       " 'interest.n.05',\n",
       " 'interest.n.05',\n",
       " 'sake.n.01',\n",
       " 'sake.n.01',\n",
       " 'sake.n.01',\n",
       " 'sake.n.01',\n",
       " 'interest.n.05',\n",
       " 'sake.n.01',\n",
       " 'interest.n.01',\n",
       " 'interest.n.06',\n",
       " 'interest.n.06',\n",
       " 'interest.n.04',\n",
       " 'interest.n.04',\n",
       " 'interest.n.01',\n",
       " 'interest.n.03',\n",
       " 'interest.n.01',\n",
       " 'interest.n.03',\n",
       " 'sake.n.01',\n",
       " 'sake.n.01',\n",
       " 'interest.n.05',\n",
       " 'sake.n.01',\n",
       " 'sake.n.01',\n",
       " 'sake.n.01',\n",
       " 'sake.n.01',\n",
       " 'sake.n.01',\n",
       " 'interest.n.01',\n",
       " 'interest.n.06',\n",
       " 'interest.n.01',\n",
       " 'pastime.n.01',\n",
       " 'interest.n.01',\n",
       " 'interest.n.01',\n",
       " 'interest.n.01',\n",
       " 'interest.v.01',\n",
       " 'interest.v.01',\n",
       " 'interest.v.01']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_senses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19ea487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c10883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5433ee35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9563d10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cc8cc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ab29f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5323923",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
